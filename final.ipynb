{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a11ea9af-533b-4d2d-b648-48f14f65d9bb",
   "metadata": {},
   "source": [
    "# Detecting Planes from Satellite Images\n",
    "- Dataset: https://www.kaggle.com/datasets/rhammell/planesnet\n",
    "- 32,000 Images\n",
    "- 200x200 Size\n",
    "- Filename Format: {label} _ {scene id} _ {longitude} _ {latitude}.png\n",
    "- label: 1 = plane, 0 = no-plane"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f695c4-c5b8-44db-8c0f-43845e2b0020",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "4c4335ef-2b0e-4914-80c7-51d38b2db930",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install ultralytics\n",
    "from ultralytics import YOLO\n",
    "import pathlib\n",
    "import tensorflow as tf\n",
    "from shutil import copyfile\n",
    "from os import makedirs, listdir, path\n",
    "import numpy as np\n",
    "\n",
    "AUTOTUNE = tf.data.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "c058eb6c-80e1-4665-949e-1925f426680f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT = 'data/plane_sat_cv/'\n",
    "NO_PLANE = ROOT + 'split_data/no_plane/'\n",
    "TEST = ROOT + 'yolo/test/'\n",
    "TRAIN = ROOT + 'yolo/train/'\n",
    "VALID = ROOT + 'yolo/valid/'\n",
    "SCENES = ROOT + 'scenes/'\n",
    "OUT = ROOT + 'yolo/'\n",
    "\n",
    "IMG_1 = SCENES + 'scene_1.png'\n",
    "IMG_2 = SCENES + 'scene_2.png'\n",
    "IMG_3 = SCENES + 'scene_3.png'\n",
    "IMG_4 = SCENES + 'scene_4.png'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16f58edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create subdirs\n",
    "def create_dirs(path):\n",
    "    dirs = ['images/', 'labels/']\n",
    "    for dir in dirs:\n",
    "        newdir = path + dir\n",
    "        makedirs(newdir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5abff6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create subdirs for train and valid\n",
    "create_dirs(TRAIN)\n",
    "create_dirs(VALID)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684af9f8-4d8a-4068-bf1b-578660823831",
   "metadata": {},
   "source": [
    "# Inital Testing\n",
    "Predicting with pre-trained model. No objects detected in the four scenes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1c26c8-b58e-477c-a9e1-545c2fb6037e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a model\n",
    "base_model = YOLO('yolov8n.pt')  # pretrained YOLOv8n model\n",
    "\n",
    "# Run batched inference on a list of images\n",
    "base_results = base_model([IMG_1, IMG_2, IMG_3, IMG_4])  # return a list of Results objects\n",
    "\n",
    "# Process results list\n",
    "for result in base_results:\n",
    "    boxes = result.boxes  # Boxes object for bounding box outputs\n",
    "    masks = result.masks  # Masks object for segmentation masks outputs\n",
    "    keypoints = result.keypoints  # Keypoints object for pose outputs\n",
    "    probs = result.probs  # Probs object for classification outputs\n",
    "    obb = result.obb  # Oriented boxes object for OBB outputs\n",
    "\n",
    "    # print(boxes, masks, keypoints, probs, obb)\n",
    "    \n",
    "    # display to screen\n",
    "    # result.show()\n",
    "    # save to disk\n",
    "    result.save(filename = OUT + 'result.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7d8ab2-6a2a-4807-be1a-804f943ea9c1",
   "metadata": {},
   "source": [
    "# Attempt to Create Model With Manual Annotations\n",
    "- Annotated three scenes manually using Roboflow\n",
    "- Testing on all four\n",
    "\n",
    "- No Detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc821f07-b6a7-481d-b158-08c67b15323a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install roboflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404d8299-7b66-47bf-abc9-9606e12a994a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from roboflow import Roboflow\n",
    "\n",
    "API_KEY = 'e8cysaNBvhp4SlHcEbn0'\n",
    "\n",
    "rf = Roboflow(api_key=API_KEY)\n",
    "project = rf.workspace(\"testbench-jd1iq\").project(\"sat_planes\")\n",
    "version = project.version(2)\n",
    "dataset = version.download(model_format=\"yolov8\", location='/home/bauen/datasets/sat_plane_robo/', overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bcb0fae-95e9-4d38-b7f9-7dac672c29c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Pretrained Model\n",
    "model = YOLO('yolov8n.pt')\n",
    "\n",
    "# Train Model\n",
    "results = model.train(data='/home/bauen/datasets/sat_plane_robo/data.yaml', epochs=20, imgsz=640)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1c369d-279e-4a09-8c48-762927362851",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = model.val()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2c5df7-e55a-49b1-a28e-261f61dadb67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run batched inference on a list of images\n",
    "# IMG_1, IMG_2, IMG_3, IMG_4\n",
    "custom_results = model([IMG_1])  # return a list of Results objects\n",
    "\n",
    "# Process results list\n",
    "for result in custom_results:\n",
    "    boxes = result.boxes  # Boxes object for bounding box outputs\n",
    "    masks = result.masks  # Masks object for segmentation masks outputs\n",
    "    keypoints = result.keypoints  # Keypoints object for pose outputs\n",
    "    probs = result.probs  # Probs object for classification outputs\n",
    "    obb = result.obb  # Oriented boxes object for OBB outputs\n",
    "\n",
    "    # print(boxes, masks, keypoints, probs, obb)\n",
    "    \n",
    "    # display to screen\n",
    "    # result.show()\n",
    "    # save to disk\n",
    "    result.save(filename = OUT + 'custom_result.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0f8195-0b08-4a5c-8285-c15e503b3266",
   "metadata": {},
   "source": [
    "# Attempting to train on small images with slightly smaller bounding boxes\n",
    "- Detects the entire image as plane"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65409e9-3d87-4c29-93d1-77d14f50c012",
   "metadata": {},
   "source": [
    "## Split into Training/Validation Sets - Create Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "444da798",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create Labelled File for Plane Images\n",
    "# Get the plane images\n",
    "data_dir = pathlib.Path(TEST).with_suffix('')\n",
    "# Find image count\n",
    "image_count = len(list(data_dir.glob('*.png')))\n",
    "image_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d95e7f90-b13c-4eae-bdf6-7e9f3100ce0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plane_ds = tf.data.Dataset.list_files(str(data_dir/'*'), shuffle=False)\n",
    "plane_ds = plane_ds.shuffle(image_count, reshuffle_each_iteration=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1af01511-73a4-4cd8-9a69-67e6032c8ee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'data/plane_sat_cv/yolo/test/1__20170618_173641_0c38__-118.40925052785828_33.94085609606337.png'\n",
      "b'data/plane_sat_cv/yolo/test/1__20151029_161054_0b0a__-118.403770832_33.9411002799.png'\n",
      "b'data/plane_sat_cv/yolo/test/1__20160714_165520_0c59__-118.407234684_33.9393855686.png'\n",
      "b'data/plane_sat_cv/yolo/test/1__20161003_213745_1_0c74__-118.430001018_33.9438317458.png'\n",
      "b'data/plane_sat_cv/yolo/test/1__20170619_180820_0f3f__-122.38370291898272_37.61474007266974.png'\n"
     ]
    }
   ],
   "source": [
    "# Output sample to check if everything is correct\n",
    "for f in plane_ds.take(5):\n",
    "  print(f.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9a26c986-baf9-4112-8a7e-78ac717463d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into training/validation sets\n",
    "val_size = int(image_count * 0.2)\n",
    "train_ds = plane_ds.skip(val_size)\n",
    "val_ds = plane_ds.take(val_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7a62f710-08f3-4c4a-83f4-adf91ed9b02c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6400\n",
      "1600\n"
     ]
    }
   ],
   "source": [
    "# Print set lengths\n",
    "print(tf.data.experimental.cardinality(train_ds).numpy())\n",
    "print(tf.data.experimental.cardinality(val_ds).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "bc9925fe-477b-4c17-a4fe-41b05389a8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_file(file_path):\n",
    "    # Label format: class_name x y width height\n",
    "    # Set x/y to center and box to 5 px inside edge of img\n",
    "    # If your boxes are in pixels, divide x_center and width by image width,\n",
    "    label = \"0 0.5 0.5 0.975 0.975\"\n",
    "    # Load the raw data from the file as a string\n",
    "    img = tf.io.read_file(file_path)\n",
    "    \n",
    "    return img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "f6bcd18d-9308-4b3c-b1c0-b00d87145958",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set `num_parallel_calls` so multiple images are loaded/processed in parallel.\n",
    "train_pairs = train_ds.map(handle_file, num_parallel_calls=AUTOTUNE)\n",
    "val_pairs = val_ds.map(handle_file, num_parallel_calls=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "e2ff2734-7d9f-4588-8388-378cc578d207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Shape:  tf.Tensor(b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\x00\\x14\\x00\\x00\\x00\\x14\\x08\\x02\\x00\\x00\\x00\\x02\\xeb\\x8aZ\\x00\\x00\\x03,IDATx\\x9c\\x05\\xc1\\tn\\x1c7\\x10\\x00\\xc0\\xeef\\x933\\xb3\\x87V+i\\xb5R\\xe2\\xa7\\xe6)y\\x80?e$\\x01\\xe2\\xc06 \\xed1\\x07\\xc9a\\x1f\\xa9\\xc2\\xaf\\x7f\\xfe\\xa1j*\\xd2Z\\x135\\x00\\xdfnw\\xcf\\xc7\\x17\\x91v\\xbd|\\xa8\\x1a3\\x03\\x11\\x07^[\\xcdyq\\xd7@\\xe4\\x0e\\x88\\xc01\\xc4\\xc8\\xa0\\x1c\\x98\\x83\\x99\\xb7&\\xad\\xe94O\"\\xab\\xa8\\xaa\\xa8\\x88\\xc4\\x98\\xba\\x98\\x08\\x10\\x11\\xdd@\\xd5\\xdc\\xc1\\xdd\\xd9L\\x1d\\xc0\\xdc\\xdd\\x9d\\x08C w(%\\x97ZL\\r\\xdcMUT\\xcdLMD\\x14\\x10\\x08)p\\x10QV3\\x00\\x00\\x00DB\\xa4\\x18\\xc9\\x1d\\xa4\\t\\xb8\\x9b)\\x02\\x01\\x91\\xaa\\xe4bHh\\xee\\x89\\xe30\\x0c1u\\xcb\\xb2p\\xdfujf\\x0e\\x8cH\\x88f\\xa6\"i\\x18\\xf6\\x9b\\xcd\\xbc\\x8c\\xd3\\x9cU\\xdd\\x1c\\x00\\xccU\\xd5LM\\x1d\\x80\\xd6\\x96\\xcb\\xcc\\x88D\\x04\\x08h\\xaa\\x0e`\\xe6\\xa2\\x9a8\\xbc>\\xbf\\x98=}\\xfb\\xfb\\xdb}\\x9a\\x01\\x03\\xa2\\x83\\x03\\x00\\xac5\\xa3;\\x10\\xb5VXU\\xdd\\x01\\t\\xcc\\xd4\\x1dZ\\x93ZK\\xad\\xd3\\xe3\\xe3\\xf1|\\xfa\\xfd\\xf36\\xde\\xa7\\xbf\\xd6\\x96C\\xe4\\xdd\\xfe\\xd8\\xa5\\xde\\xad\\r\\xfd&\\xc6d\\xbark\\r\\x00\\x89(\\x10\\x95\\x92K]E|m\\xebe\\x9c\\xceo\\xfc|\\xfa\\xf2\\xcf\\x8f_y.\\x9b\\x18\\xcfo_^\\x9fO\\xe8\\x0e\\xaef.\\xad\\xb0\\xa9\\x99Y?\\x0c\\xc7\\xc3\\xe1b~\\xb9\\xdenS^\\xcaZ5=\\x1c\\x7f{=\\x9d\\xfa\\xcd\\xfe\\xe7\\xaf\\x0f(P\\x8b\\xccs6\\x95y\\xba\\x94\\x9a#\\x11\\x03@\\xa9E\\xcc\\x1ev\\x8fC\\xff\\x80\\xf0y\\xbd\\xfe\\x98k\\xcb+\\x1d\\xff\\xfd\\x1e\\x98\\xcd\\x9aYC\\x0c\\x97\\xcf\\xdbt\\xbfJ\\xaby\\xb9\\xb9\\xdb\\xeb\\xf3+;\\x98\\x98\\xd6\\xbc\\xdc\\xc7\\xfb\\xc3\\xfe\\xb0\\xd9\\xee\\x1e\\x0f\\xc7\\xbe5s\\x8c\\xe0\\xa8\\xf6|8\\x8e\\xc7\\x1b\\xc7\\xae\\xd4\\\\r\\x05\\x137@b3e\"\\xec\\xbb\\xae\\xa9\\x1a@\\x88\\xbc\\xdbm\\xde\\xcf\\xe7\\xb5\\xad\\xa2\\xfa\\xe5\\xed|zz\\x9a\\xe7[\\xd7\\xa5\\x18C\\x9f\\xfa\\xb5\\xba67B@\\x1c\\x97\\x85\\x11)\\x04R\\x95Z\\xf3\\xe5\\xfaQ\\xd6<\\x0c\\x1d\\x05Ru\\x03\\xb8\\xcf\\xf7q\\xbe\"\\xd8n;<\\x1e\\x8e%w\\xcb4\\xe6<!\\x91\\xb9\\xb3\\x99\\xa1C\\x0c\\x81\\xd0J\\x99\\xa5I\\x1a\\xba\\xbe\\xeb\\xc0\\x01]\\xc7\\xf1^\\xea\\xca\\x1c90\\x82\\xa5\\xc8\\xdawMj\\x8cl\\xe6\\x8c\\x80\\xcc\\x1cBr\\x87\\xb9\\x94\\xda\\x040\\xa7\\x14#\\x85e\\x99jkk-\\x01\\xad\\xad\\xf9z\\xc9\\x84\\xe8\\x8e\\x80\\x14c\\x04\\x00\\xee\\xfb\\xb4\\x1b\\x86\\x94\\xfai\\xc9M\\xd4\\x01\\xbbD\\xfba\\xe8S,\\xb5\\xd42\\x99T5\\xb5\\xac\\x1cB\\xe4DD)bJ\\x89C\\xe2\\xddn\\xd7q\\xecR\\xef\\x8e\\xf32\\x03\\xe2v\\x18\\xce/\\xa7\\x97\\xa7\\xe3\\x7f?\\xbf_\\xc6\\x8b\\xbb!\\x12\\x80#\"\\x12m7\\xc3~\\xfb\\xb4\\xdb\\x1dR\\xb7\\xe1.u`\\xb6,s\\xce\\xd9\\xcd8\\xb0\\xaa\\xcde\\xdc\\xcb\\x06B\\x8cq\\x18\\xfa\\xe0\\x00\\x88\\x109\\x0e}\\xff~:\\xbd\\x9f\\xde8F\\x03`Ss\\xf7R\\xeb\\x94g\\xa2@\\x1cD\\xe5\\xf3z\\x9dsi\\x02\\x0e\\xcc\\x11]\\x85\\x03\\xc5\\x98\\x10q\\xc9\\xf9z\\x9f\\xdc\\xdb\\xb8L\\xec\\xe0D\\x14\\xbb\\xae\\x131\\x00p\\'D3\\x98\\xe6\\x8a\\x14B\\xe0\\xa6\\xaa\\xe6\\x80\\x8aR\\xc5\\xc2U\\xf56\\x8e\\xe3t\\x11\\x91\\xff\\x01num\\x03\\x12\\xd2\\x9bE\\x00\\x00\\x00\\x00IEND\\xaeB`\\x82', shape=(), dtype=string)\n",
      "Label:  b'0 0.5 0.5 0.975 0.975'\n"
     ]
    }
   ],
   "source": [
    "for img, label in train_pairs.take(1):\n",
    "  print(\"Image Shape: \", img)\n",
    "  print(\"Label: \", label.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f1f0c7e4-955a-4d54-8cac-6f7483fd23ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_pairs(pairs, base):\n",
    "    idx = 0\n",
    "    for img, label in pairs:\n",
    "        # Create Image Path\n",
    "        img_path = path.join(base + 'images/', '%s.jpg'%idx)\n",
    "        # Create Label Path\n",
    "        label_path = path.join(base + 'labels/', '%s.txt'%idx)\n",
    "        # Write Image\n",
    "        tf.io.write_file(img_path, img)\n",
    "        # Write Label\n",
    "        tf.io.write_file(label_path, label)\n",
    "\n",
    "        idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "346b23b3-cd57-4dd0-b95b-c84f531ffb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write Training Images/Labels\n",
    "write_pairs(train_pairs, TRAIN)\n",
    "write_pairs(val_pairs, VALID)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6655c09-087e-43f3-90b6-d01657e121cb",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "d212215c-364c-4788-84c2-6c57deddb0d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.2.6 ðŸš€ Python-3.10.12 torch-2.3.0+cu121 CUDA:0 (NVIDIA GeForce RTX 3080 Ti, 12288MiB)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8n.pt, data=data/plane_sat_cv/yolo/data.yaml, epochs=20, time=None, patience=100, batch=16, imgsz=200, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train27, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=/home/bauen/data-mining/runs/detect/train27\n",
      "Overriding model.yaml nc=80 with nc=1\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    751507  ultralytics.nn.modules.head.Detect           [1, [64, 128, 256]]           \n",
      "Model summary: 225 layers, 3011043 parameters, 3011027 gradients, 8.2 GFLOPs\n",
      "\n",
      "Transferred 319/355 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir /home/bauen/data-mining/runs/detect/train27', view at http://localhost:6006/\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n",
      "WARNING âš ï¸ imgsz=[200] must be multiple of max stride 32, updating to [224]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/bauen/data-mining/data/plane_sat_cv/yolo/tr\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /home/bauen/data-mining/data/plane_sat_cv/yolo/train/labels.cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/bauen/data-mining/data/plane_sat_cv/yolo/vali\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /home/bauen/data-mining/data/plane_sat_cv/yolo/valid/labels.cache\n",
      "Plotting labels to /home/bauen/data-mining/runs/detect/train27/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added âœ…\n",
      "Image sizes 224 train, 224 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1m/home/bauen/data-mining/runs/detect/train27\u001b[0m\n",
      "Starting training for 20 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       1/20     0.782G     0.2677     0.6498     0.9499         5\n",
      "                 Class     Images  Instances      Box(P          /home/bauen/data-mining/.venv/lib/python3.10/site-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n",
      "                 Class     Images  Instances      Box(P          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1600       1600          1          1      0.995       0.97\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       2/20     0.738G     0.2003     0.2868     0.9102         4\n",
      "                 Class     Images  Instances      Box(P          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1600       1600          1          1      0.995      0.995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       3/20      0.74G     0.1846     0.2538     0.9081         4\n",
      "                 Class     Images  Instances      Box(P          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1600       1600      0.999      0.997      0.995      0.994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       4/20     0.738G      0.164      0.219      0.906         5\n",
      "                 Class     Images  Instances      Box(P          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1600       1600          1          1      0.995      0.796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       5/20     0.738G     0.1439     0.1936     0.9027         5\n",
      "                 Class     Images  Instances      Box(P          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1600       1600          1          1      0.995      0.995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       6/20     0.738G     0.1255     0.1667      0.896         4\n",
      "                 Class     Images  Instances      Box(P          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1600       1600          1          1      0.995      0.995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       7/20     0.738G     0.1173     0.1563     0.8944         4\n",
      "                 Class     Images  Instances      Box(P          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1600       1600      0.928      0.987      0.977      0.977\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       8/20     0.738G     0.1114     0.1469     0.8944         5\n",
      "                 Class     Images  Instances      Box(P          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1600       1600      0.999          1      0.995      0.911\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       9/20     0.738G      0.103     0.1445     0.8923         4\n",
      "                 Class     Images  Instances      Box(P          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1600       1600          1          1      0.995      0.995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      10/20     0.738G    0.09424     0.1298     0.8907         5\n",
      "                 Class     Images  Instances      Box(P          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1600       1600          1          1      0.995      0.995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing dataloader mosaic\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      11/20     0.736G     0.0831     0.1145     0.9004         1\n",
      "                 Class     Images  Instances      Box(P          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1600       1600          1          1      0.995      0.995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      12/20     0.736G    0.06073     0.0669     0.8948         1\n",
      "                 Class     Images  Instances      Box(P          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1600       1600          1          1      0.995       0.99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      13/20     0.736G    0.05548    0.06126      0.898         1\n",
      "                 Class     Images  Instances      Box(P          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1600       1600          1          1      0.995      0.995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      14/20     0.736G    0.04906    0.05514     0.8979         1\n",
      "                 Class     Images  Instances      Box(P          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1600       1600          1          1      0.995      0.995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      15/20     0.736G    0.04355     0.0493     0.8948         1\n",
      "                 Class     Images  Instances      Box(P          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1600       1600          1          1      0.995      0.995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      16/20     0.738G    0.04042    0.04584     0.8919         1\n",
      "                 Class     Images  Instances      Box(P          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1600       1600          1          1      0.995      0.995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      17/20     0.736G     0.0378    0.04103     0.8931         1\n",
      "                 Class     Images  Instances      Box(P          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1600       1600          1          1      0.995      0.995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      18/20     0.736G    0.03317    0.03734     0.8908         1\n",
      "                 Class     Images  Instances      Box(P          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1600       1600          1          1      0.995      0.995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      19/20     0.736G    0.03085    0.03347      0.893         1\n",
      "                 Class     Images  Instances      Box(P          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1600       1600          1          1      0.995      0.995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      20/20     0.736G    0.02811    0.03035     0.8892         1\n",
      "                 Class     Images  Instances      Box(P          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1600       1600          1          1      0.995      0.995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "20 epochs completed in 0.229 hours.\n",
      "Optimizer stripped from /home/bauen/data-mining/runs/detect/train27/weights/last.pt, 6.2MB\n",
      "Optimizer stripped from /home/bauen/data-mining/runs/detect/train27/weights/best.pt, 6.2MB\n",
      "\n",
      "Validating /home/bauen/data-mining/runs/detect/train27/weights/best.pt...\n",
      "Ultralytics YOLOv8.2.6 ðŸš€ Python-3.10.12 torch-2.3.0+cu121 CUDA:0 (NVIDIA GeForce RTX 3080 Ti, 12288MiB)\n",
      "Model summary (fused): 168 layers, 3005843 parameters, 0 gradients, 8.1 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bauen/data-mining/.venv/lib/python3.10/site-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n",
      "                 Class     Images  Instances      Box(P          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1600       1600          1          1      0.995      0.995\n",
      "Speed: 0.1ms preprocess, 0.6ms inference, 0.0ms loss, 1.0ms postprocess per image\n",
      "Results saved to \u001b[1m/home/bauen/data-mining/runs/detect/train27\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Load Pretrained Model\n",
    "model = YOLO('yolov8n.pt')\n",
    "\n",
    "# Train Model\n",
    "results = model.train(data=OUT + 'data.yaml', epochs=20, imgsz=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3da0699-5f6e-4591-ad99-47c45e953d64",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "d14914a3-3332-47c3-a164-be9a589cc4ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.2.6 ðŸš€ Python-3.10.12 torch-2.3.0+cu121 CUDA:0 (NVIDIA GeForce RTX 3080 Ti, 12288MiB)\n",
      "Model summary (fused): 168 layers, 3005843 parameters, 0 gradients, 8.1 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/bauen/data-mining/data/plane_sat_cv/yolo/vali\u001b[0m\n",
      "                 Class     Images  Instances      Box(P          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1600       1600          1          1      0.995      0.995\n",
      "Speed: 0.1ms preprocess, 0.9ms inference, 0.0ms loss, 1.0ms postprocess per image\n",
      "Results saved to \u001b[1m/home/bauen/data-mining/runs/detect/train272\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "metrics = model.val()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147b2474-e988-495f-8318-9c15ed32d0f8",
   "metadata": {},
   "source": [
    "## Testing Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "32df8672-88eb-45cd-92ec-e839377aa7ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 224x224 1 planes, 24.0ms\n",
      "Speed: 2.1ms preprocess, 24.0ms inference, 9.0ms postprocess per image at shape (1, 3, 224, 224)\n"
     ]
    }
   ],
   "source": [
    "# Run batched inference on a list of images\n",
    "# IMG_1, IMG_2, IMG_3, IMG_4\n",
    "custom_results = model([IMG_1])  # return a list of Results objects\n",
    "\n",
    "# Process results list\n",
    "for result in custom_results:\n",
    "    boxes = result.boxes  # Boxes object for bounding box outputs\n",
    "    masks = result.masks  # Masks object for segmentation masks outputs\n",
    "    keypoints = result.keypoints  # Keypoints object for pose outputs\n",
    "    probs = result.probs  # Probs object for classification outputs\n",
    "    obb = result.obb  # Oriented boxes object for OBB outputs\n",
    "\n",
    "    # print(boxes, masks, keypoints, probs, obb)\n",
    "    \n",
    "    # display to screen\n",
    "    # result.show()\n",
    "    # save to disk\n",
    "    result.save(filename = OUT + 'custom_result.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8630d08-57d4-457c-936a-e7a363526565",
   "metadata": {},
   "source": [
    "# Creating composite images with plane and no-plane images\n",
    "Not finished. The idea was to create composite images by combining majority of no_plane images with a handful of plane images. The labels could then be easily know based on where in the composite image it was placed.\n",
    "\n",
    "This may work, but the time limit was reached before this could be achieved."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e69e76-0047-4296-be90-25ad3370dc79",
   "metadata": {},
   "source": [
    "## Split into Training/Validation Sets - Create Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "7b668cc1-5216-4420-a0f8-efc2d45d205f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8000"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create Labelled File for Plane Images\n",
    "# Get the plane images\n",
    "data_dir = pathlib.Path(TEST).with_suffix('')\n",
    "no_plane_dir = pathlib.Path(NO_PLANE).with_suffix('')\n",
    "# Find image count\n",
    "image_count = len(list(data_dir.glob('*.png')))\n",
    "image_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "52bcb1e7-4993-4cab-a1fc-a8f2cb27e219",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24000"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get no_plane list\n",
    "no_plane_list = list(no_plane_dir.glob('*'))\n",
    "len(no_plane_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c93f4389-e924-43c1-8a72-64f1a5e41e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "plane_ds = tf.data.Dataset.list_files(str(data_dir/'*'), shuffle=False)\n",
    "plane_ds = plane_ds.shuffle(image_count, reshuffle_each_iteration=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "20f31889-4858-4dcf-9082-422e682b92ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'data/plane_sat_cv/yolo/test/1__20170618_173641_0c38__-118.40925052785828_33.94085609606337.png'\n",
      "b'data/plane_sat_cv/yolo/test/1__20151029_161054_0b0a__-118.403770832_33.9411002799.png'\n",
      "b'data/plane_sat_cv/yolo/test/1__20160714_165520_0c59__-118.407234684_33.9393855686.png'\n",
      "b'data/plane_sat_cv/yolo/test/1__20161003_213745_1_0c74__-118.430001018_33.9438317458.png'\n",
      "b'data/plane_sat_cv/yolo/test/1__20170619_180820_0f3f__-122.38370291898272_37.61474007266974.png'\n"
     ]
    }
   ],
   "source": [
    "# Output sample to check if everything is correct\n",
    "for f in plane_ds.take(5):\n",
    "  print(f.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1a87ae5a-9613-4bd5-8cb9-3ad6f42f58c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into training/validation sets\n",
    "val_size = int(image_count * 0.2)\n",
    "train_ds = plane_ds.skip(val_size)\n",
    "val_ds = plane_ds.take(val_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "a374fd39-3d79-494d-a9dd-69e8608a750a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6400 1600\n",
      "640.0 160.0\n"
     ]
    }
   ],
   "source": [
    "# Print set lengths\n",
    "train_len = tf.data.experimental.cardinality(train_ds).numpy()\n",
    "val_len = tf.data.experimental.cardinality(val_ds).numpy()\n",
    "print(train_len, val_len)\n",
    "# Get num of image to create\n",
    "train_num = train_len / 10\n",
    "val_num = val_len / 10\n",
    "print(train_num, val_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "7332da70-d9bf-46b0-a58a-a491d305a3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set `num_parallel_calls` so multiple images are loaded/processed in parallel.\n",
    "train_pairs = train_ds.map(handle_file, num_parallel_calls=AUTOTUNE)\n",
    "val_pairs = val_ds.map(handle_file, num_parallel_calls=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "4421054c-a1dc-421a-98de-cc1e6d4ed488",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_file(batch):\n",
    "    # Create new image with the 10 passed planes and 90 random no_planes\n",
    "    comb_img = np.zeros((2000, 2000, 3), dtype=np.uint8)\n",
    "    y = 0\n",
    "    x = 0\n",
    "    for file_path in batch:\n",
    "        img = tf.io.read_file(file_path)\n",
    "        img = tf.io.decode_image(img)\n",
    "        img = img.numpy()\n",
    "        comb_img = np.add(comb_img, img)\n",
    "        # Create Label: iterate by 200 pixels horizontally and by one row height, then add more rows\n",
    "        # Label format: class_name x y width height\n",
    "        label_y = ((y + 200) - 100) / 200\n",
    "        label_x = ((x + 200) - 100) / 200\n",
    "        label =  f\"0 {label_y} {label_x} 1 1\"\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff02e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab 10 images at a time\n",
    "for i in range(0, train_len, 10):\n",
    "    batch = train_ds.skip(i).take(10)\n",
    "    handle_file(batch)\n",
    "    write_pairs(train_pairs, TRAIN)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
